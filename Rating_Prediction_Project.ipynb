{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCRAPING DATA OF REVIEWS FROM AMAZON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating python program to search desired keywords one by one and saving links in open list prod_url\n",
    "\n",
    "# creating open list to save href links of all products \n",
    "prod_url = []\n",
    "\n",
    "# Keywords for search to which we need to scrape the data.\n",
    "keywords = ['Laptops', 'Phones', 'Headphones', 'Smart Watches', 'Cameras', 'Printers', 'Monitors','Home theater', 'Router']\n",
    "\n",
    "# Location of chrome web driver and launching it\n",
    "driver = webdriver.Chrome(\"C:/Users/User/Downloads/chromedriver_win32 (1)/chromedriver\")\n",
    "\n",
    "# website from which we will scrape data\n",
    "driver.get(\"https://www.amazon.in/\") \n",
    "\n",
    "initial_url = driver.current_url\n",
    "\n",
    "# loop for sending keywords to website and scraping href\n",
    "for i in keywords:\n",
    "    # Finding search input and giving input\n",
    "    search = driver.find_element_by_id(\"twotabsearchtextbox\").send_keys(i)\n",
    "        \n",
    "    # clicking on search button\n",
    "    search_btn = driver.find_element_by_xpath(\"//div[@class='nav-search-submit nav-sprite']\")\n",
    "    search_btn.click()\n",
    "    # giving time to load the webpage completely to avoid errors\n",
    "    time.sleep(5)\n",
    "    # finding href of each products mentioned in keywords\n",
    "    for j in driver.find_elements_by_xpath(\"//h2[@class = 'a-size-mini a-spacing-none a-color-base s-line-clamp-2']/a\"):\n",
    "        prod_url.append(j.get_attribute('href'))\n",
    "    # returning to initial url    \n",
    "    driver.get(initial_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking length of prod_url\n",
    "len(prod_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "WebDriverException",
     "evalue": "Message: unknown error: cannot determine loading status\nfrom disconnected: received Inspector.detached event\n  (Session info: chrome=97.0.4692.71)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-ea6c5b19f813>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mprod_url\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# opening each links of prod_url one by one\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[0mLoads\u001b[0m \u001b[0ma\u001b[0m \u001b[0mweb\u001b[0m \u001b[0mpage\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcurrent\u001b[0m \u001b[0mbrowser\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m         \"\"\"\n\u001b[1;32m--> 333\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGET\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'url'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    334\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[0;32m    323\u001b[0m                 response.get('value', None))\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    240\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'alert'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mWebDriverException\u001b[0m: Message: unknown error: cannot determine loading status\nfrom disconnected: received Inspector.detached event\n  (Session info: chrome=97.0.4692.71)\n"
     ]
    }
   ],
   "source": [
    "# opening each 226 prod_url and scraping links of all reviews href \n",
    "# open list to save all reviews href\n",
    "rev_cli = []\n",
    "\n",
    "# loop for opening all links stored in prod_url\n",
    "for j in prod_url:\n",
    "    # opening each links of prod_url one by one\n",
    "    driver.get(j)\n",
    "    \n",
    "    try:\n",
    "        # finding links from all webpage by xpath\n",
    "        for k in driver.find_elements_by_xpath(\"//a[@id = 'acrCustomerReviewLink']\"):\n",
    "            rev_cli.append(k.get_attribute('href'))\n",
    "    # if all reviews link is not available, it will continue to next page,so avoiding NoSuchElementException \n",
    "    except NoSuchElementException as e:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking length of open list\n",
    "len(rev_cli)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now opening each links as stored in list above\n",
    "# creating open list for storing all reviews href\n",
    "all_reviews = []\n",
    "\n",
    "for l in rev_cli:\n",
    "    # opening each links as stored in cli_rev open list\n",
    "    driver.get(l)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    try:\n",
    "        # finding links from all webpage by xpath\n",
    "        all_rev = driver.find_element_by_xpath(\"//div[@id = 'reviews-medley-footer']/div[2]/a\")\n",
    "        all_reviews.append(all_rev.get_attribute('href'))\n",
    "        # if all reviews link is not available, it will continue to next page,so avoiding NoSuchElementException \n",
    "    except NoSuchElementException as e:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking length of all reviews link\n",
    "len(all_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally opening all_reviews link and scraping ratings and reviews data from all webpages.\n",
    "# creating open list to save scraped data\n",
    "ratings = []\n",
    "reviews = []\n",
    "\n",
    "# opening each link to begin the scaping\n",
    "for m in all_reviews:\n",
    "    driver.get(m)\n",
    "    time.sleep(15)\n",
    "    \n",
    "    # loop to open link till page 5 and scraping data of reviews and ratings of products\n",
    "    for n in range(0,5):\n",
    "        for o in driver.find_elements_by_xpath(\"//div[@class = 'a-section review aok-relative']/div/div/div[2]/a[1]\"):\n",
    "            ratings.append(o.get_attribute('title').replace(' out of 5 stars',''))\n",
    "            \n",
    "        for p in driver.find_elements_by_xpath(\"//div[@class = 'a-section review aok-relative']/div/div/div[4]/span/span\"):\n",
    "            reviews.append(p.text)\n",
    "                        \n",
    "        try:\n",
    "            next_page = driver.find_element_by_xpath(\"//ul[@class = 'a-pagination']/li[2]/a\")\n",
    "            driver.get(next_page.get_attribute('href'))\n",
    "        except NoSuchElementException as e:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving scraped data into dataframe.\n",
    "search = pd.DataFrame({}) \n",
    "search['Ratings'] = ratings[:20442]\n",
    "search['Full_review'] = reviews[:20442]\n",
    "search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving scraped data in csv format\n",
    "search.to_csv('Review_rating.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rating Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#importing nltk libraries\n",
    "import nltk\n",
    "#nltk.download('all')\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import FreqDist\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score,precision_score, confusion_matrix, accuracy_score,classification_report\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB,BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the data into a dataframe\n",
    "df = pd.read_csv(\"Review_rating.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Information:\n",
    "    \n",
    "Review_title : title of the review\n",
    "    \n",
    "Review_text : content of the review text\n",
    "    \n",
    "Ratings : ratings out of 5 stars\n",
    "    \n",
    "At first I will drop the unwanted column Unnamed: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = 'Unnamed: 0',inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the shape\n",
    "print(\"Shape :\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great our data set is having 77550 rows and three columns now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking column names\n",
    "print(\"Columns :\", df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By checking all above observations we can say the data contains three columns Review_title , Reiew_text and Ratings. All columns are object type. Here Ratings column is our target column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set is having lot of missing values and we will drop these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization of missing values\n",
    "sns.set(rc={'figure.figsize':(11.8,8.27)})\n",
    "sns.heatmap(data=df.isnull())\n",
    "plt.xticks(rotation=90, fontsize=10)\n",
    "plt.yticks(rotation=0, fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the rows where review_title and Reiew_text has null values\n",
    "df.dropna(subset=['Review_title','Reiew_text'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for null values again\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great we have successfully removed all the missing data from our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets check the shape of our data set now\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look at our target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Ratings'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at these entries in target column we came to know that we need to replace the string entries to respective values(stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Ratings'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Review_title and Reiew_text into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joining Review text and title\n",
    "df['Review'] = df['Review_title'].map(str)+' '+df['Reiew_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets have a look at our data now\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing text in first three rows from Review column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Review'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Review'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Review'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here I am defining a function to replace some of the contracted words to their full form and removing urls and some unwanted text \n",
    "def decontracted(text):\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"don’t\", \"do not\", text)\n",
    "    text = re.sub(r\"can't\", \"can not\", text)\n",
    "    text = re.sub(r\"im \", \"i am\", text)\n",
    "    text = re.sub(r\"yo \", \"you \",text)\n",
    "    text = re.sub(r\"doesn’t\", \"does not\",text)\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'s\", \" is\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "    text = re.sub(r\"<br>\", \" \", text)\n",
    "    text = re.sub(r'http\\S+', '', text) #removing urls\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lowercasing\n",
    "df['Review'] = df['Review'].apply(lambda x : x.lower())\n",
    "\n",
    "df['Review'] = df['Review'].apply(lambda x : decontracted(x))\n",
    "\n",
    "#removing punctuations\n",
    "df['Review'] = df['Review'].str.replace('[^\\w\\s]','')\n",
    "df['Review'] = df['Review'].str.replace('\\n',' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets have a look at our text again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Review'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Review'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove stopwords\n",
    "stop = stopwords.words('english')\n",
    "df['Review'] = df['Review'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Review'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Review'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that now we are having text without any stop words in it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining functiom to convert nltk tag to wordnet tags\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining function to lemmatize our text\n",
    "def lemmatize_sentence(sentence):\n",
    "    #tokenize the sentence & find the pos tag\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x : (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatize_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            lemmatize_sentence.append(word)\n",
    "        else:\n",
    "            lemmatize_sentence.append(lemmatizer.lemmatize(word,tag))\n",
    "    return \" \".join(lemmatize_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Review'] = df['Review'].apply(lambda x : lemmatize_sentence(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Review'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Review'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Normalization - Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Noice removal\n",
    "def scrub_words(text):\n",
    "    #remove html markup\n",
    "    text = re.sub(\"(<.*?>)\", \"\", text)\n",
    "    #remove non-ascii and digits\n",
    "    text = re.sub(\"(\\\\W)\", \" \", text)\n",
    "    text = re.sub(\"(\\\\d)\", \"\", text)\n",
    "    #remove white space\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Review'] = df['Review'].apply(lambda x : scrub_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Review'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Review'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally I have defined a function scrub_words for removing the noise from the text. It will remove any html markups, digits and white spaces from the text. We can understand it by looking at first two row's text from review column\n",
    "\n",
    "Now We have did all the text-processing steps and got required input for our model. We will go for EDA now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating column for word counts in the text\n",
    "df['Review_WC'] = df['Review'].apply(lambda x: len(str(x).split(' ')))\n",
    "df[['Review_WC','Review']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#density plot and histogram of all word count\n",
    "sns.distplot(df['Review_WC'], hist = True, kde = True,\n",
    "            bins = int(180/5), color = 'darkblue',\n",
    "            hist_kws = {'edgecolor':'black'},\n",
    "            kde_kws = {'linewidth':4})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the above histogram we can clearly observe that most of our text is having the number of words in the range of 0 to 200, But some of the reviews are too lengthy which may act like outliers in our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating column for character counts in the text\n",
    "df['Review_CC'] = df['Review'].str.len()\n",
    "df[['Review_CC','Review']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#density plot and histogram of all character count\n",
    "sns.distplot(df['Review_CC'], hist = True, kde = True,\n",
    "            bins = int(180/5), color = 'darkblue',\n",
    "            hist_kws = {'edgecolor':'black'},\n",
    "            kde_kws = {'linewidth':4})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above plot represents histogram for character count of review text, which is quite similar to the histogram of word count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing Outliers\n",
    "\n",
    "As we know that some of the review are too lengthy I am removing those reviews from the dats as outliers using z_score method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, by removing outliers we are loosing only arround 1000 entries which is acceptable here for getting beter results for our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting histograms for word count and character counts again after removing outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#density plot and histogram of all word count\n",
    "sns.distplot(df['Review_WC'], hist = True, kde = True,\n",
    "            bins = int(180/5), color = 'darkblue',\n",
    "            hist_kws = {'edgecolor':'black'},\n",
    "            kde_kws = {'linewidth':4})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#density plot and histogram of all word count\n",
    "sns.distplot(df['Review_CC'], hist = True, kde = True,\n",
    "            bins = int(180/5), color = 'darkblue',\n",
    "            hist_kws = {'edgecolor':'black'},\n",
    "            kde_kws = {'linewidth':4})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After plotting histograms for word counts and character counts after removing outliers we can see now we are with good range of number of words and characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets have a look at our data set\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the count of target column\n",
    "sns.countplot(df['Ratings'])\n",
    "print(df.Ratings.value_counts())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the above count plot for our target varible(Ratings) we can say that the data set is having most number of reviews rated as 5 star. and very less number of reviews rated as 2.\n",
    "\n",
    "Which will cause the Imbalance problem for our model.\n",
    "\n",
    "So I am selecting equal number of reviews of each rating as a input for our model\n",
    "\n",
    "For that first I will shuffle the dataset so that we can select data from both web-sites\n",
    "\n",
    "Then I will select equal number of data of every category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffling the data set\n",
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selct data from every category\n",
    "df1 = df[df['Ratings']==1][0:7356]\n",
    "df2 = df[df['Ratings']==2][0:7356]\n",
    "df3 = df[df['Ratings']==3][0:7356]\n",
    "df4 = df[df['Ratings']==4][0:7356]\n",
    "df5 = df[df['Ratings']==5][0:7356]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining all the dataframes into one and shuffling them \n",
    "df = pd.concat([df1,df2,df3,df4,df5],ignore_index=True)\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets Check the count of target column again\n",
    "sns.countplot(df['Ratings'])\n",
    "print(df.Ratings.value_counts())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we have balanced our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 30 most frequently occuring words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to plot most frequent terms\n",
    "def freq_words(x, terms = 30):\n",
    "    all_words = ' '.join([text for text in x])\n",
    "    all_words = all_words.split()\n",
    "    fdist = FreqDist(all_words)\n",
    "    words_df = pd.DataFrame({'word':list(fdist.keys()),\n",
    "                             'count':list(fdist.values())})\n",
    "    #selecting top 30 most freq words\n",
    "    d = words_df.nlargest(columns = 'count', n = terms)\n",
    "    plt.figure(figsize = (20,10))\n",
    "    ax = sns.barplot(data = d, x='count', y='word')\n",
    "    ax.set(ylabel = 'Word')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_words(df['Review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 30 Rare words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to plot least frequent terms\n",
    "def rare_words(x, terms = 30):\n",
    "    all_words = ' '.join([text for text in x])\n",
    "    all_words = all_words.split()\n",
    "    fdist = FreqDist(all_words)\n",
    "    words_df = pd.DataFrame({'word':list(fdist.keys()),\n",
    "                             'count':list(fdist.values())})\n",
    "    #selecting top 30 most freq words\n",
    "    d = words_df.nsmallest(columns = 'count', n = terms)\n",
    "    plt.figure(figsize = (20,10))\n",
    "    ax = sns.barplot(data = d, x='count', y='word')\n",
    "    ax.set(ylabel = 'Word')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_words(df['Review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "stopwords = set(STOPWORDS)\n",
    "def show_wordcloud(data, title = None):\n",
    "    wordcloud = WordCloud(\n",
    "                    background_color='white',\n",
    "                    stopwords = stopwords,\n",
    "                    max_words = 500,\n",
    "                    max_font_size = 40,\n",
    "                    scale = 3,\n",
    "                    random_state = 1).generate(str(data))\n",
    "    fig = plt.figure(1, figsize=(15,15))\n",
    "    plt.axis('off')\n",
    "    if title:\n",
    "        fig.suptitle(title, fontsize=20)\n",
    "        fig.subplots_adjust(top=2.3)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words for rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_wordcloud(df1['Review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_wordcloud(df2['Review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_wordcloud(df3['Review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_wordcloud(df4['Review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_wordcloud(df5['Review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separating features and labels\n",
    "x = df['Review']\n",
    "y = df['Ratings']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting text into vectors using TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the n_gram tfidf vectorizer(Word vectors)\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "                                sublinear_tf = True,\n",
    "                                strip_accents = 'unicode',\n",
    "                                analyzer = 'word',\n",
    "                                token_pattern = r'\\w{1,}',\n",
    "                                stop_words = 'english',\n",
    "                                ngram_range = (1,3),\n",
    "                                max_features = 100000)\n",
    "word_vectorizer.fit(x)\n",
    "train_word_features = word_vectorizer.transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Character vectors\n",
    "char_vectorizer = TfidfVectorizer(\n",
    "                                sublinear_tf = True,\n",
    "                                strip_accents = 'unicode',\n",
    "                                analyzer = 'char',\n",
    "                                stop_words = 'english',\n",
    "                                ngram_range = (2,6),\n",
    "                                max_features = 50000)\n",
    "char_vectorizer.fit(x)\n",
    "train_char_features = char_vectorizer.transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I will combine both word vectors and character vectors as input for our model\n",
    "train_features = hstack([train_char_features,train_word_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "x_train, x_test, y_train, y_test = train_test_split(train_features, y, test_size = 0.25, random_state = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets check the shapes of traning and test data\n",
    "print(\"x_train\", x_train.shape)\n",
    "print(\"x_test\", x_test.shape)\n",
    "print(\"y_train\", y_train.shape)\n",
    "print(\"y_test\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the algorithms\n",
    "rf = RandomForestClassifier()\n",
    "svc = LinearSVC()\n",
    "lr = LogisticRegression(solver='lbfgs')\n",
    "mnb = MultinomialNB()\n",
    "xgb = XGBClassifier(verbosity=0)\n",
    "bnb = BernoulliNB()\n",
    "lgb = LGBMClassifier()\n",
    "sgd = SGDClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a function to train and test the model with evaluation\n",
    "def BuiltModel(model):\n",
    "    print('*'*30+model.__class__.__name__+'*'*30)\n",
    "    model.fit(x_train,y_train)\n",
    "    y_pred = model.predict(x_train)\n",
    "    pred = model.predict(x_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test,pred)*100\n",
    "\n",
    "    print(f\"Accuracy Score:\", accuracy)\n",
    "    print(\"---------------------------------------------------\")\n",
    "\n",
    "    #confusion matrix & classification report\n",
    "    \n",
    "    print(f\"CLASSIFICATION REPORT : \\n {classification_report(y_test,pred)}\")\n",
    "    print(f\"Confusion Matrix : \\n {confusion_matrix(y_test,pred)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and testing of various algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in [lr,svc,bnb,mnb,sgd,rf,xgb]:\n",
    "    BuiltModel(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, among all these algorithms 4 are giving good accuracies: LogisticRegression, LinearSVC, SGDClassifier and RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val(model):\n",
    "    print('*'*30+model.__class__.__name__+'*'*30)\n",
    "    scores = cross_val_score(model,train_features,y, cv = 3).mean()*100\n",
    "    print(\"Cross validation score :\", scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am checking cross-validation score only for those algorithms which are giving us better accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in [lr,svc,sgd,rf]:\n",
    "    cross_val(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great all our algorithms are giving good cv scores with very less difference in accuracy and cv-scores. Among these algorithms I am selecting LinearSVC as best suitable algorithm for our final model as it is giving least difference in accuracy and cv score with higher accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HyperParameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets selects different parameters for tuning\n",
    "grid_params = {\n",
    "                'penalty':['l2'],\n",
    "                'loss':['hinge','squared_hinge'],\n",
    "                'multi_class': ['ovr'],\n",
    "                'intercept_scaling':[2,3],\n",
    "                'dual':[True,False],\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model with given parameters using GridSearchCV\n",
    "GCV =  GridSearchCV(svc, grid_params, cv = 3, verbose=10)\n",
    "GCV.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCV.best_params_       #printing the best parameters found by GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training and testing our final model with above parameters\n",
    "model = LinearSVC(dual = True, intercept_scaling = 2, loss = 'hinge', multi_class = 'ovr', penalty = 'l2')\n",
    "model.fit(x_train,y_train) #fitting data to model\n",
    "pred = model.predict(x_test)\n",
    "accuracy = accuracy_score(y_test,pred)*100\n",
    "\n",
    "#printing accuracy score\n",
    "print(\"Accuracy Score :\", accuracy)\n",
    "\n",
    "#printing Confusion matrix\n",
    "print(f\"\\nConfusion Matrix : \\n {confusion_matrix(y_test,pred)}\\n\")\n",
    "\n",
    "#printing Classification report\n",
    "print(f\"\\nCLASSIFICATION REPORT : \\n {classification_report(y_test,pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great; after doing hyperparameter tuning we have got improved accuracy score for our final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(model,\"Rating_Prediction.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally I am saving this model into a .pkl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
